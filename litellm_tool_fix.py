# litellm_tool_fix.py
"""
LiteLLM callback to fix null tool descriptions for Harmony protocol compatibility.

The vLLM Harmony protocol requires every ToolDescription.description to be a
non-null string. Some tools generated by the Claude Code CLI have description=None,
causing a pydantic ValidationError. This callback patches those before the request
reaches vLLM.

Uses `async_pre_call_hook` (mutable, fires BEFORE provider transformation) to patch
descriptions, and `log_pre_api_call` (read-only, fires AFTER transformation) to log
the final outgoing tool schemas for debugging.

Register in your LiteLLM proxy config:

    litellm_settings:
      callbacks: ["litellm_tool_fix.HarmonyToolFixer"]

Log output is written to `litellm_tool_fix.log` (via the `harmony_tool_fixer` logger).
"""
import logging
from litellm.integrations.custom_logger import CustomLogger

logger = logging.getLogger("harmony_tool_fixer")
logger.setLevel(logging.DEBUG)

# File handler so logs persist even if LiteLLM's root logger isn't configured
_handler = logging.FileHandler("litellm_tool_fix.log")
_handler.setFormatter(logging.Formatter("%(asctime)s %(levelname)s %(message)s"))
logger.addHandler(_handler)


class HarmonyToolFixer(CustomLogger):
    async def async_pre_call_hook(self, user_api_key_dict, cache, data, call_type):
        """Patch null tool descriptions before LiteLLM translates the request.

        Handles both formats that may appear at this stage:
        - Anthropic flat: {"name": "Bash", "description": ..., "input_schema": {...}}
        - OpenAI nested: {"type": "function", "function": {"name": "Bash", ...}}
        """
        tools = data.get("tools")
        if not tools:
            return data

        patched = []
        for tool in tools:
            # OpenAI nested format
            func = tool.get("function")
            if func and func.get("description") is None:
                name = func.get("name", "tool")
                func["description"] = f"{name} - no description available"
                patched.append(name)

            # Anthropic flat format (has "name" at top level, no "function" key)
            if "name" in tool and "function" not in tool:
                if tool.get("description") is None:
                    name = tool.get("name", "tool")
                    tool["description"] = f"{name} - no description available"
                    patched.append(name)

        if patched:
            logger.warning(f"Patched null descriptions for: {', '.join(patched)}")
        else:
            logger.debug(f"All {len(tools)} tools already have descriptions")

        return data

    def log_pre_api_call(self, model, messages, kwargs):
        """Log the final outgoing tool schemas after LiteLLM translation.

        This is a read-only hook (fires AFTER transformation), used purely for
        debugging to see what vLLM actually receives.
        """
        tools = kwargs.get("tools") or kwargs.get("optional_params", {}).get("tools")
        if not tools:
            return

        tool_summary = []
        for tool in tools:
            func = tool.get("function", tool)  # handle both formats
            tool_summary.append({
                "name": func.get("name", "?"),
                "has_description": func.get("description") is not None,
                "schema_type": "openai" if "function" in tool else "anthropic",
                "param_key": (
                    "parameters" if "parameters" in func else
                    "input_schema" if "input_schema" in func else
                    "none"
                ),
            })

        logger.info(f"Outgoing tools for {model}: {tool_summary}")
