# Resources: Recursive Language Models

Complete collection of links for the "Recursive Language Models" paper.

---

## Official Paper

### Primary Sources
- **arXiv Abstract**: https://arxiv.org/abs/2512.24601
- **arXiv PDF**: https://arxiv.org/pdf/2512.24601
- **arXiv HTML Version**: https://arxiv.org/html/2512.24601v2
- **arXiv TeX Source**: https://arxiv.org/src/2512.24601
- **HuggingFace Paper Page**: https://huggingface.co/papers/2512.24601

### Paper Metadata
- **Published**: December 31, 2025 (v1); January 28, 2026 (v2)
- **Pages**: 9 (33 with appendix)
- **License**: CC BY 4.0
- **Citations**: Track on [Google Scholar](https://scholar.google.com/scholar?q=Recursive+Language+Models+Zhang+Kraska+Khattab)

---

## Code and Models

### Official Implementation
- **Main Repository**: https://github.com/alexzhang13/rlm
  - Stars: 2.6k+
  - Forks: 486
  - License: MIT
  - Installation: `pip install rlms`

- **Minimal Implementation**: https://github.com/alexzhang13/rlm-minimal
  - Bare-bones reference implementation
  - Good for understanding core concepts

### Pre-trained Model
- **RLM-Qwen3-8B-v0.1**: https://huggingface.co/mit-oasys/rlm-qwen3-8b-v0.1
  - First native recursive language model
  - 8B parameters
  - Fine-tuned on 1,000 trajectories
  - 28.3% improvement over base Qwen3-8B

### Community Implementations
- **fullstackwebdev/rlm_repl**: https://github.com/fullstackwebdev/rlm_repl
  - Proof-of-concept implementation
  - Based on paper methodology

- **ysz/recursive-llm**: https://github.com/ysz/recursive-llm
  - Alternative implementation
  - Focuses on 100k+ token processing
  - Variable-based context storage

- **brainqub3/claude_code_RLM**: https://github.com/brainqub3/claude_code_RLM
  - Claude Code integration
  - Scaffold for RLM workflows

---

## Author Resources

### Alex L. Zhang
- **Personal Website**: https://alexzhang13.github.io/
- **Official Blog Post**: https://alexzhang13.github.io/blog/2025/rlm/
- **Google Scholar**: https://scholar.google.com/citations?user=rtCr0q4AAAAJ&hl=en
- **Twitter/X**: (Check personal website for current handle)
- **GitHub**: https://github.com/alexzhang13

### Omar Khattab
- **Personal Website**: https://omarkhattab.com/
- **Twitter/X**: https://x.com/lateinteraction
- **Google Scholar**: https://scholar.google.com/citations?user=Lwr5ozgAAAAJ&hl=en
- **GitHub**: https://github.com/okhat
- **Notable Projects**:
  - DSPy: https://github.com/stanfordnlp/dspy
  - ColBERT: https://github.com/stanford-futuredata/ColBERT

### Tim Kraska
- **MIT CSAIL Profile**: https://people.csail.mit.edu/kraska/
- **Google Scholar**: https://scholar.google.com/citations?user=F_vNLjEAAAAJ
- **AWS Research**: https://www.amazon.science/author/tim-kraska
- **Notable Work**: Learned Indexes (2018)

---

## Blog Posts and Explanations

### Official Blog
- **Alex L. Zhang's Deep Dive**: https://alexzhang13.github.io/blog/2025/rlm/
  - Core concepts and technical implementation
  - Key results and emergent strategies
  - Author's perspective on significance

### Industry Analysis
- **Prime Intellect — "RLMs: the paradigm of 2026"**: https://www.primeintellect.ai/blog/rlm
  - Positioning as transformative for LLM agents
  - Context efficiency and scalable thinking
  - Research roadmap for 2026

- **MarkTechPost — "From MIT's Blueprint to Prime Intellect's RLMEnv"**: https://www.marktechpost.com/2026/01/02/recursive-language-models-rlms-from-mit-blueprint-to-prime-intellects-rlmenv-for-long-horizon-llm-agents/
  - Overview and practical applications
  - RLMEnv platform for distributed execution

### Technical Deep Dives
- **Navendu Pottekkat — "Notes on Recursive Language Models"**: https://navendu.me/posts/recursive-language-models/
  - REPL implementation details
  - Practical benefits and context efficiency
  - Developer perspective

- **Pietro Bolcato (Medium) — "Infinite Context that Works"**: https://medium.com/@pietrobolcato/recursive-language-models-infinite-context-that-works-174da45412ab
  - Comprehensive explanation of mechanics
  - Performance comparisons with vanilla LLMs
  - Visual diagrams and examples

- **Stéphane (Medium) — "Technical Deep Dive into Infinite Context"**: https://medium.com/@comeback01/recursive-language-models-rlms-a-technical-deep-dive-into-the-infinite-context-paradigm-85b5b43373fc
  - Implementation details and architecture
  - Code examples and patterns

### Accessible Explanations
- **Shreya Sri (Medium) — "Explained Through Classic Algorithms"**: https://medium.com/ai-simplified-in-plain-english/recursive-language-models-056eea0d4762
  - ELI5-style simplified explanation
  - Analogies to classic CS concepts

- **Vinod Polinati (Medium) — "The Real Fix for Long-Context AI in 2026?"**: https://medium.com/@vinodpolinati/recursive-language-models-could-this-be-the-real-fix-for-long-context-ai-in-2026-070328df9329
  - Problem/solution framing
  - Positioning among alternatives

- **Alex Merced (Substack) — "What Are Recursive Language Models?"**: https://amdatalakehouse.substack.com/p/what-are-recursive-language-models
  - Introduction for data professionals
  - Use cases in data engineering

### Applied Examples
- **Towards Data Science — "Going Beyond the Context Window"**: https://towardsdatascience.com/going-beyond-the-context-window-recursive-language-models-in-action/
  - Real-world demonstration
  - Processing ~1.5M characters (386k tokens)
  - Practical implementation tips

- **Avi Chawla (Daily Dose of Data Science)**: https://blog.dailydoseofds.com/p/recursive-language-models
  - Data science perspective
  - Simplified approach explanation

- **Introl Blog — "RLM Context Management 2026"**: https://introl.com/blog/recursive-language-models-rlm-context-management-2026
  - Focus on context management strategies
  - Enterprise applications

---

## News Coverage

### Technology News
- **InfoQ — "MIT's Recursive Language Models Improve Performance"**: https://www.infoq.com/news/2026/01/mit-recursive-lm/
  - Technical news coverage
  - Industry implications

- **BDTechTalks — "A New Framework for Infinite Context in LLMs"**: https://bdtechtalks.com/2026/01/26/recursive-language-models/
  - Memory management analogy
  - Practical applications breakdown
  - Interview insights

- **The Neuron Daily — "Recursive Language Models?!"**: https://www.theneurondaily.com/p/recursive-language-models
  - Framed as elegant solution to memory limitations
  - Genuine progress vs incremental improvements

### Research Aggregators
- **Emergent Mind**: https://www.emergentmind.com/papers/2512.24601
  - Comprehensive tracking (380 tweets, 166k+ likes)
  - ELI14 explanations
  - 29 identified research questions
  - Knowledge gaps and glossary
  - Practical applications breakdown

- **ArXivIQ (Substack)**: https://arxiviq.substack.com/p/recursive-language-models
  - Paper summary and analysis
  - Key takeaways

---

## Community Discussions

### Hacker News
- **Primary Thread (Jan 4, 2026)**: https://news.ycombinator.com/item?id=46475395
  - 160+ points
  - Active technical discussion
  - Critiques and counterpoints
  - Community debate on novelty

- **Earlier Thread (Oct 2025)**: https://news.ycombinator.com/item?id=45596059
  - Initial presentation discussion
  - Early reactions

- **Related: "Less is more: Recursive reasoning with tiny networks"**: https://news.ycombinator.com/item?id=45506268
  - Related conversation on recursive approaches

### Social Media
- **Twitter/X Tracking**: 380+ tweets, 166k+ likes (per Emergent Mind)
- **Follow Key Voices**:
  - Omar Khattab: https://x.com/lateinteraction
  - MIT CSAIL: https://x.com/MIT_CSAIL
  - #RLM hashtag for discussions

### Forums and Communities
- **Reddit Discussions**: Search r/MachineLearning, r/LanguageTechnology for "Recursive Language Models"
- **GitHub Discussions**: https://github.com/alexzhang13/rlm/discussions
- **Discord**: Check RLM GitHub README for community server links

---

## Conference Talks and Videos

### Presentations
- **Recursive Language Models w/ Alex Zhang (Luma Talk)**: https://luma.com/ksfwzxem
  - Zoom talk by lead author
  - Recent results and insights beyond blog post
  - Q&A with researchers

### Expected Future Talks
Watch these conferences in 2026 for follow-up presentations:
- **ICLR 2026** (May) — likely workshop or poster
- **ICML 2026** (July) — potential main track or tutorial
- **NeurIPS 2026** (December) — watch for citations and follow-ups
- **ACL/EMNLP 2026** — NLP applications

---

## Related Work

### Foundational Papers

#### By Same Authors
- **DSPy: Compiling Declarative Language Model Calls**: https://arxiv.org/abs/2310.03714
  - Framework philosophy underlying RLMs
  - Omar Khattab et al., 2023

- **ColBERT: Efficient and Effective Passage Search**: https://arxiv.org/abs/2004.12832
  - Dense retrieval foundations
  - Khattab & Zaharia, SIGIR 2020

- **The Case for Learned Index Structures**: https://arxiv.org/abs/1712.01208
  - ML replacing traditional algorithms
  - Kraska et al., 2018

#### Context Understanding
- **Lost in the Middle**: https://arxiv.org/abs/2307.03172
  - Documents context rot problem
  - Liu et al., NeurIPS 2023

- **RULER Benchmark**: https://arxiv.org/abs/2404.06654
  - Long-context evaluation methodology
  - Hsieh et al., 2024

- **LongBench**: https://arxiv.org/abs/2308.14508
  - Standard benchmark for long context
  - Bai et al., 2023

#### Reasoning Approaches
- **Chain-of-Thought Prompting**: https://arxiv.org/abs/2201.11903
  - Decomposition for reasoning
  - Wei et al., NeurIPS 2022

- **ReAct: Synergizing Reasoning and Acting**: https://arxiv.org/abs/2210.03629
  - Interactive reasoning philosophy
  - Yao et al., ICLR 2023

### Complementary Technologies
- **DSPy Framework**: https://github.com/stanfordnlp/dspy
  - Natural integration point for RLMs
  - Systematic LLM programming

- **ColBERT v2**: https://arxiv.org/abs/2112.01488
  - Efficient retrieval for RAG alternatives

---

## Tools and Platforms

### Sandbox Options
- **Docker**: https://www.docker.com/
  - Recommended for production isolation

- **Modal**: https://modal.com/
  - Serverless code execution at scale

- **E2B**: https://e2b.dev/
  - Code execution platform for AI

- **Prime Intellect RLMEnv**: https://www.primeintellect.ai/blog/rlm
  - Distributed RLM execution environment
  - Research platform (in development)

- **Daytona**: https://www.daytona.io/
  - Cloud development environments

### Model Providers
- **OpenAI API**: https://platform.openai.com/
  - GPT-5, GPT-5-mini, GPT-4o support

- **Anthropic**: https://www.anthropic.com/
  - Claude 3.5 support

- **OpenRouter**: https://openrouter.ai/
  - Unified access to many models

- **HuggingFace**: https://huggingface.co/
  - Self-hosted model access
  - RLM-Qwen3-8B hosting

---

## Datasets and Benchmarks

### Used in Paper
- **S-NIAH (Synthetic Needle-in-a-Haystack)**: Custom benchmark (check paper appendix)
- **OOLONG**: Custom long-context reasoning benchmark (check paper)
- **BrowseComp-Plus**: Extended version of BrowseComp benchmark
- **CodeQA (LongBench-v2)**: https://github.com/THUDM/LongBench
- **LongBenchPro**: Used for training RLM-Qwen3-8B

### Related Benchmarks
- **RULER**: https://github.com/hsiehjackson/RULER
  - Comprehensive long-context evaluation

- **LongBench**: https://github.com/THUDM/LongBench
  - Standard multi-task benchmark

- **InfiniteBench**: https://github.com/OpenBMB/InfiniteBench
  - Extremely long context testing

---

## Learning Resources

### For Beginners
1. **Start Here**: Read `summary.md` in this research folder
2. **Watch**: Luma talk by Alex Zhang (when available)
3. **Read**: Alex Zhang's blog post (accessible introduction)
4. **Try**: Install `pip install rlms` and run quick start example

### For Practitioners
1. **Read**: `practical-takeaways.md` in this research folder
2. **Explore**: Official GitHub repository examples
3. **Study**: Community blog posts on implementation
4. **Experiment**: Try RLM on your specific use case

### For Researchers
1. **Read**: Full paper + appendix carefully
2. **Study**: DSPy and ColBERT papers for context
3. **Review**: `learning-path.md` Level 5 for open questions
4. **Engage**: Join GitHub Discussions and HN threads

---

## Stay Updated

### Follow
- **Omar Khattab on X**: https://x.com/lateinteraction (most active)
- **MIT CSAIL News**: https://www.csail.mit.edu/news
- **ArXiv cs.CL**: https://arxiv.org/list/cs.CL/recent (for citations)
- **RLM GitHub**: https://github.com/alexzhang13/rlm (releases and updates)

### Subscribe
- **Alex Zhang's Blog**: https://alexzhang13.github.io/blog/ (RSS available)
- **Emergent Mind**: https://www.emergentmind.com/papers/2512.24601 (paper tracking)
- **The Neuron Daily**: https://www.theneurondaily.com/ (AI news)

### Search
- **Google Scholar Alert**: Set alert for "Recursive Language Models" + authors
- **GitHub Trending**: Watch for new implementations
- **HackerNews**: Search periodically for new discussions

---

## Citation

If you use RLMs in your work, cite as:

```bibtex
@article{zhang2025recursive,
  title={Recursive Language Models},
  author={Zhang, Alex L. and Kraska, Tim and Khattab, Omar},
  journal={arXiv preprint arXiv:2512.24601},
  year={2025}
}
```

For the model:

```bibtex
@misc{rlm-qwen3-8b,
  author = {Zhang, Alex L. and Kraska, Tim and Khattab, Omar},
  title = {RLM-Qwen3-8B-v0.1},
  year = {2026},
  publisher = {HuggingFace},
  howpublished = {\url{https://huggingface.co/mit-oasys/rlm-qwen3-8b-v0.1}}
}
```

---

## Quick Reference

**Most Important Links**:
1. Paper: https://arxiv.org/abs/2512.24601
2. Code: https://github.com/alexzhang13/rlm
3. Model: https://huggingface.co/mit-oasys/rlm-qwen3-8b-v0.1
4. Blog: https://alexzhang13.github.io/blog/2025/rlm/
5. Discussion: https://news.ycombinator.com/item?id=46475395

**Install and Try**:
```bash
pip install rlms
```

**Get Help**:
- Issues: https://github.com/alexzhang13/rlm/issues
- Discussions: https://github.com/alexzhang13/rlm/discussions
- Email authors: Check paper for contact info

---

**Last Updated**: February 2026
**Maintained By**: L7 Agent Research System
**Research Folder**: `research_output/paper-recursive-language-models/`
